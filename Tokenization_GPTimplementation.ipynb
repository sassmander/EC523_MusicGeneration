{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "xqq3M6lhGoJV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2ahGKiflE2x",
        "outputId": "a4441b4b-db2c-4aab-ffc0-53c2a13cfe94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Shareddrives/EC523_Project/CodeWorkspace\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "%cd /content/drive/Shareddrives/EC523_Project/CodeWorkspace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for Tokenizer"
      ],
      "metadata": {
        "id": "1Ttf-DDXGtoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYIGFEMak_vN"
      },
      "outputs": [],
      "source": [
        "!pip install miditok\n",
        "!pip install tokenizers\n",
        "!pip install setuptools_rust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R68tuRUFsW3x"
      },
      "outputs": [],
      "source": [
        "import miditok\n",
        "import miditoolkit\n",
        "from miditok import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the tokenizer"
      ],
      "metadata": {
        "id": "wi8e2Jr0Gyq1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNelE_kSsP3Q"
      },
      "outputs": [],
      "source": [
        "pitch_range = range(21, 109)\n",
        "beat_res = {(0, 4): 8, (4, 12): 4}\n",
        "nb_velocities = 32\n",
        "additional_tokens = {'Chord': True, 'Rest': False, 'Tempo': True, 'Program': False, 'TimeSignature': False,\n",
        "                     'rest_range': (2, 8),  # (half, 8 beats)\n",
        "                     'nb_tempos': 32,  # nb of tempo bins\n",
        "                     'tempo_range': (40, 250)}  # (min, max)\n",
        "special_tokens = [\"PAD\", \"BOS\", \"EOS\", \"MASK\"]\n",
        "tokenizer = REMI(pitch_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCmIBtzQux8"
      },
      "source": [
        "\n",
        "Code to Move MAESTRO Files into respective folders to separate between training, validation, and testing\n",
        "\n",
        "Don't run twice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QzIHVQb1pSL"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "src_dir = '/content/drive/Shareddrives/EC523_Project/maestro-v3.0.0/'\n",
        "dst_dir_train = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/MAESTRO/train'\n",
        "dst_dir_valid = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/MAESTRO/validation'\n",
        "dst_dir_test = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/MAESTRO/test'\n",
        "\n",
        "with open('/content/drive/Shareddrives/EC523_Project/maestro-v3.0.0/maestro-v3.0.0.csv', 'r') as csvfile:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(csvfile)\n",
        "    for i, row in enumerate(reader):\n",
        "        if row[2] == 'train':\n",
        "          filename = src_dir+row[4]\n",
        "          if filename[-4:] == '.wav':\n",
        "            print('Skipping ', i)\n",
        "          else:\n",
        "            shutil.copy2(filename, dst_dir_train)\n",
        "        if row[2] == 'validation':\n",
        "          filename = src_dir+row[4]\n",
        "          if filename[-4:] == '.wav':\n",
        "            print('Skipping ', i)\n",
        "          else:\n",
        "            shutil.copy2(filename, dst_dir_valid)\n",
        "        if row[2] == 'test':\n",
        "          filename = src_dir+row[4]\n",
        "          if filename[-4:] == '.wav':\n",
        "            print('Skipping ', i)\n",
        "          else:\n",
        "            shutil.copy2(filename, dst_dir_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOg-laPGQ1W_"
      },
      "source": [
        "Code to train base tokenizer on training dataset, No need to run again, but can run on validation and testing once trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-l24Cf4lF_p"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "midi_paths = list(Path('MAESTRO','train').glob('**/*.midi'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk0C1HqBuqXY",
        "outputId": "67e51a24-b7fb-4c62-be55-40401bab92da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing MIDIs (CodeWorkspace/tokens_BPE_train): 100%|██████████| 962/962 [08:02<00:00,  1.99it/s]\n",
            "Applying BPE to dataset: 100%|██████████| 961/961 [00:41<00:00, 22.88it/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer.tokenize_midi_dataset(midi_paths, '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE_train')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYziZ0t-8GGU"
      },
      "outputs": [],
      "source": [
        "# Set vocab size of the BPE tokenizer\n",
        "vocab_size = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iUCLrNjwuYD",
        "outputId": "30bd792a-99ed-401f-83ae-1823e954a5b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading token files: 100%|██████████| 961/961 [00:05<00:00, 179.21it/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer.learn_bpe(\n",
        "    vocab_size=vocab_size,\n",
        "    tokens_paths=list(Path('tokens_noBPE').glob(\"**/*.json\")),\n",
        "    out_dir=Path('tokens_BPE'),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV8giaVRx3Rw",
        "outputId": "1115a027-d92a-4196-b285-7284d9356036"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Applying BPE to dataset: 100%|██████████| 961/961 [00:46<00:00, 20.65it/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer.apply_bpe_to_dataset(Path('tokens_noBPE'), Path('tokens_BPE'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QkBnaq-sk0u",
        "outputId": "c0baa7d2-61cb-4812-afc6-d6c1274cdc06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing MIDIs (CodeWorkspace/tokens_BPE_test): 100%|██████████| 177/177 [01:03<00:00,  2.80it/s]\n",
            "Applying BPE to dataset: 100%|██████████| 177/177 [00:05<00:00, 33.61it/s]\n"
          ]
        }
      ],
      "source": [
        "# Using trained tokenizer, converting test tokens to BPE versions\n",
        "from pathlib import Path\n",
        "midi_paths = list(Path('MAESTRO','test').glob('**/*.midi'))\n",
        "tokenizer.tokenize_midi_dataset(midi_paths, '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfJI8cVytEUo",
        "outputId": "90ab23ed-0794-4bd0-afc5-93a95dc45dce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing MIDIs (CodeWorkspace/tokens_BPE_valid): 100%|██████████| 137/137 [00:54<00:00,  2.51it/s]\n",
            "Applying BPE to dataset: 100%|██████████| 137/137 [00:04<00:00, 29.34it/s]\n"
          ]
        }
      ],
      "source": [
        "# Using trained tokenizer, converting validation tokens to BPE versions\n",
        "from pathlib import Path\n",
        "midi_paths = list(Path('MAESTRO','validation').glob('**/*.midi'))\n",
        "tokenizer.tokenize_midi_dataset(midi_paths, '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE_valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WTT5RERQTKE"
      },
      "source": [
        "Saving and Loading Trained Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_9e8NUw1K-x"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Run after training tokenizer on training set\n",
        "tokenizer.save_params('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/REMItokenizer.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtgeJmYPQaKG"
      },
      "outputs": [],
      "source": [
        "# Run to use the same tokenizer again\n",
        "tokenizer.load_params('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/REMItokenizer.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSCX8bIppMZN",
        "outputId": "93af937f-e880-44cf-cdee-83e3076ed8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ý': 220, 'J': 41, '\\x86': 101, '\\x8a£': 314, 'T\\x8e\\x9e': 335, 'w': 86, '\\x94\\x9f': 251, 'R\\x8e\\x9e': 366, 'I': 40, 'º': 153, '\\x91\\x9e': 226, '\\\\\\x90\\x9e': 430, 'Y\\x8d\\x9e': 364, 'O\\x8d\\x9e': 336, '½': 156, 'R\\x90\\x9e': 420, '\\x8e¤': 395, 'î': 205, 's': 82, 'R': 49, '\\x87¤': 346, 'M\\x8a\\x9e': 385, '\\x8c¤': 370, 'V\\x8c\\x9e': 389, 'Q\\x90\\x9e': 407, 'µ': 148, 'U\\x90\\x9e': 458, '*': 9, 'U\\x8e\\x9e': 426, 'G': 38, '\\x88\\x9e': 232, 'ç': 198, 'x': 87, 'p': 79, 'X': 55, '\\x94\\xa0': 279, '\\x82': 97, '¸': 151, 'c': 66, '\\x96': 117, 'Y\\x8f\\x9e': 347, 'L\\x8c\\x9e': 462, ']\\x8f\\x9e': 369, '8': 23, '[': 58, '«': 138, 'W\\x8e\\x9e': 398, 'ä': 195, '\\x8e': 109, 'ã': 194, '\\x8c\\x9e': 225, '\\x85£': 495, '\\x90\\xa0': 255, '\\\\': 59, '\\x96\\x9e': 249, '¯': 142, ']\\x90\\x9e': 387, '`\\x90\\x9e': 396, '\\x89¤': 339, '#': 2, '\\x92¡': 291, '\\x85¡': 313, 'Û': 186, '\\x90¡': 285, '£': 130, 'Ì': 171, 'U\\x8f\\x9e': 435, 'R\\x8b\\x9e': 381, '\\x86£': 328, 'H\\x8f\\x9e': 472, '\\x85¢': 359, '\\x98': 119, '×': 182, 'N\\x8d\\x9e': 463, 'S': 50, 'U': 52, 'P': 47, 'Z\\x8f\\x9e': 441, '\\x88¡': 276, '\\x87\\x9f': 256, '\\x9f': 126, '\\x93¡': 302, 'W\\x90\\x9e': 417, '[\\x91\\x9e': 355, 'Ë': 170, '/': 14, '\\x93¢': 325, '©': 136, '[\\x8f\\x9e': 331, 'Q\\x8a\\x9e': 429, 'á': 192, 'v': 85, '\\x8b\\x9f': 245, 'O\\x8f\\x9e': 372, '\\x86\\x9e': 246, '\\x90': 111, '\\x91¡': 286, '¼': 155, 'S\\x8d\\x9e': 482, '\\x87¡': 284, ',': 11, '\\x89¡': 273, '\\x91¢': 307, 'ú': 217, 'Z\\x90\\x9e': 474, 'W\\x8f\\x9e': 410, 'M\\x8d\\x9e': 367, 'X\\x8e\\x9e': 399, '³': 146, '\\x9d': 124, '\\x83\\x9e': 277, '\\x8b¢': 298, '\\x91\\x9f': 237, 'O\\x91\\x9e': 457, ']\\x8e\\x9e': 390, '3': 18, '\\x8d\\x9e': 223, '\\x8b\\x9e': 227, '\\x95\\x9e': 240, '\\x8d\\x9f': 241, '\\x80': 95, '%â': 300, '\\x8b¡': 278, 'K\\x8e\\x9e': 464, '\\x8f¡': 283, 'M\\x8f\\x9e': 405, '\\x93\\x9e': 230, 'N\\x8f\\x9e': 498, 'm': 76, 'Õ': 180, 'Å': 164, '\\x92\\x9f': 242, '\\x88\\xa0': 268, '\\x8c¢': 297, '\\x88': 103, '\\x8d\\xa0': 260, 'U\\x8c\\x9e': 497, 'D': 35, 'ò': 209, '%á': 274, '.': 13, ')': 8, '\\x81\\x9e': 310, 'R\\x8f\\x9e': 393, 'R\\x8a\\x9e': 436, '\\x88¢': 293, '%à': 257, 'ß': 190, ']\\x8d\\x9e': 437, ']\\x91\\x9e': 431, 'õ': 212, '%ß': 244, 'à': 191, '\\x90\\x9e': 224, 'T\\x8f\\x9e': 330, '\\x91£': 350, 'Ø': 183, '\\x86\\x9f': 271, '\\x94¡': 309, 'u': 84, '\\x93\\xa0': 270, 'j': 73, 'X\\x8c\\x9e': 473, '\\x8a¡': 275, '\\x8d¤': 378, '\\x92\\xa0': 264, '¿': 158, '\\x8c': 107, '¶': 149, 'J\\x8b\\x9e': 394, 'M': 44, 'ô': 211, '\\x89£': 312, 'b\\x8f\\x9e': 492, '\\x8a\\x9f': 248, ']': 60, '\\x8a\\x9e': 229, 'O\\x90\\x9e': 401, '\\x84\\x9f': 317, '\\x8f¢': 304, '\\x97\\x9f': 337, '»': 154, 'P\\x8e\\x9e': 427, 'H\\x8a\\x9e': 438, 'Î': 173, \"'\": 6, '\\x97\\x9e': 269, '\\x8a¢': 295, '¬': 139, 'B': 33, '±': 144, 'ó': 210, '2': 17, '\\x97': 118, '·': 150, '\\x86¢': 308, 'Þ': 189, '\\x95\\xa0': 296, 'Q\\x8f\\x9e': 368, '\\x82\\x9e': 292, '\\x96\\xa0': 324, 'J\\x8e\\x9e': 391, '\\x8f¤': 419, 'Ê': 169, 'T\\x8b\\x9e': 375, '^\\x90\\x9e': 408, '\\x87': 102, '÷': 214, 'Ý': 188, 'n': 77, '\\x8e\\xa0': 259, '\\x8a\\xa0': 263, '\\x84¡': 400, 'R\\x8c\\x9e': 373, '\\x89\\x9e': 231, '9': 24, '\\x84': 99, 'y': 88, 'Y': 56, '\\x88£': 311, '@': 31, 'N': 45, '%Þ': 234, 'O': 46, '1': 16, '\\x95': 116, '+': 10, 'Ã': 162, '\\x90£': 329, ';': 26, '^\\x91\\x9e': 433, '\\x87¥': 459, '\\xad': 140, '¾': 157, 'Ò': 177, 'W': 54, '²': 145, 'K\\x8c\\x9e': 465, 'T\\x91\\x9e': 402, 'Z\\x8e\\x9e': 477, 'S\\x8f\\x9e': 485, 'å': 196, '\\x94\\x9e': 233, 'â': 193, 'L': 43, '\\x8b¤': 362, '\\x92': 113, 'Æ': 165, 'Q\\x8d\\x9e': 358, '0': 15, '\\x89\\xa0': 265, 'H\\x8e\\x9e': 424, 'Â': 161, '\\x8d¡': 281, '\\x8f£': 326, 'Ï': 174, '\\x8f\\xa0': 258, '5': 20, '\\x8c¡': 280, '4': 19, 'Í': 172, '`\\x91\\x9e': 406, 'z': 89, 'o': 78, 'ð': 207, '\\x86\\xa0': 287, '\\x8c£': 320, 'Y\\x91\\x9e': 382, '\\x88¤': 334, '&': 5, '\\x9c': 123, '|': 91, '\\xa0': 127, '§': 134, '\\x93\\x9f': 247, '\\x8c\\xa0': 261, 'M\\x90\\x9e': 461, 'R\\x91\\x9e': 494, '\\x93': 114, 'U\\x91\\x9e': 490, '\\x8d£': 321, '[\\x90\\x9e': 333, 'r': 81, 'P\\x8f\\x9e': 450, 'K\\x8d\\x9e': 452, 'J\\x8a\\x9e': 454, 'N\\x8c\\x9e': 466, '\\x89\\x9f': 250, '\\x85\\x9f': 289, '¡': 128, '\\x88¥': 418, '\\x84\\xa0': 361, '%': 4, 'ü': 219, '\"': 1, '¤': 131, 'ñ': 208, '\\x85\\xa0': 306, 'O\\x8c\\x9e': 332, '\\x7f\\x9e': 439, 'Q\\x8c\\x9e': 357, '\\x8e\\x9e': 221, '\\x8f\\x9e': 222, 'K\\x8b\\x9e': 486, 'V': 53, 'ù': 216, '\\x98\\x9e': 315, 'Q': 48, 'Ñ': 176, 'ê': 201, '\\x87\\x9e': 239, 'T\\x90\\x9e': 354, '\\x83': 98, '¥': 132, 'í': 204, 'Q\\x8b\\x9e': 392, 'P\\x8d\\x9e': 412, '\\x8f\\x9f': 236, '\\x85': 100, '`': 63, 'O\\x89\\x9e': 434, '\\x93£': 447, '<': 27, 'O\\x8a\\x9e': 383, 'F': 37, '\\x92\\x9e': 228, '%%': 484, '`\\x8d\\x9e': 489, 'N\\x8e\\x9e': 480, '\\x8b\\xa0': 262, '\\x8e¢': 303, '7': 22, '\\x92£': 403, '\\x94': 115, 'ï': 206, '\\x8a¥': 442, 'T\\x8d\\x9e': 327, '_': 62, 'Q\\x91\\x9e': 479, '~': 93, '\\x8e¡': 282, '[\\x8e\\x9e': 338, '-': 12, '®': 141, '°': 143, 'l': 75, '\\x8d': 108, 'Ô': 179, 'W\\x91\\x9e': 456, 'À': 159, 'È': 167, '{': 90, '\\x84\\x9e': 267, '\\x8d¥': 499, '}': 92, 'O\\x8e\\x9e': 348, 'H': 39, '\\x8e\\x9f': 238, '\\x9e': 125, '¢': 129, '\\x92¢': 318, 'h': 71, 'd': 67, 'Ö': 181, '%ä': 397, '(': 7, '\\x8c¥': 488, '\\x85\\x9e': 253, 'V\\x8e\\x9e': 341, 'Y\\x8e\\x9e': 344, 't': 83, 'S\\x8e\\x9e': 471, '\\\\\\x8e\\x9e': 453, '\\x8a¤': 351, 'Ç': 166, '\\x8e£': 322, 'Q\\x8e\\x9e': 360, 'E': 36, '!': 0, '\\x81': 96, 'Á': 160, 'H\\x8b\\x9e': 384, '\\x90¢': 305, 'Z\\x8d\\x9e': 496, '\\x96\\x9f': 288, '^\\x8e\\x9e': 432, 'Ù': 184, '=': 28, '[\\x8c\\x9e': 449, 'Y\\x90\\x9e': 356, '^\\x8f\\x9e': 411, '´': 147, '\\x95\\x9f': 266, '\\x91\\xa0': 254, 'ì': 203, '\\x89¢': 294, '[\\x8d\\x9e': 371, 'V\\x90\\x9e': 374, 'N\\x8b\\x9e': 483, 'Ó': 178, 'X\\x90\\x9e': 423, '\\x7f': 94, '¦': 133, 'f': 69, '¨': 135, '\\x86¡': 290, 'É': 168, '6': 21, 'Y\\x92\\x9e': 481, 'b': 65, 'L\\x8b\\x9e': 487, '[\\x92\\x9e': 448, '>': 29, 'Z': 57, 'V\\x8b\\x9e': 443, '\\x8f': 110, '\\x8b£': 319, 'T\\x8c\\x9e': 345, '\\x87\\xa0': 272, 'P\\x90\\x9e': 469, '\\x89': 104, '\\x9a': 121, ':': 25, 'g': 70, '\\x99': 120, 'Ú': 185, 'Ð': 175, 'U\\x8d\\x9e': 446, 'T': 51, '\\x91': 112, '\\x9b': 122, 'é': 200, 'ö': 213, 'e': 68, 'M\\x8b\\x9e': 365, 'J\\x8f\\x9e': 460, 'L\\x8d\\x9e': 467, 'ë': 202, 'A': 32, '?': 30, 'Ü': 187, '\\x8c\\x9f': 243, 'P\\x8c\\x9e': 415, '`\\x8f\\x9e': 388, '\\x89¥': 428, 'W\\x8d\\x9e': 444, 'b\\x90\\x9e': 493, 'X\\x8f\\x9e': 404, '\\\\\\x8f\\x9e': 422, 'k': 74, '$': 3, 'J\\x8c\\x9e': 377, '\\x95¡': 340, '\\\\\\x91\\x9e': 478, 'i': 72, 'M\\x89\\x9e': 425, '\\x91¤': 476, 'M\\x8e\\x9e': 380, 'ø': 215, '\\x8b': 106, 'H\\x8d\\x9e': 409, 'V\\x91\\x9e': 413, 'è': 199, 'J\\x8d\\x9e': 376, 'V\\x8d\\x9e': 353, '\\x88\\x9f': 252, '\\x90¤': 455, '\\x87¢': 301, 'T\\x8a\\x9e': 445, 'ª': 137, 'æ': 197, '\\x8d¢': 299, 'M\\x8c\\x9e': 352, '%ã': 323, 'a': 64, 'Y\\x8c\\x9e': 421, '~\\x9e': 491, 'C': 34, '\\x8a': 105, 'K': 42, 'H\\x8c\\x9e': 386, 'q': 80, '\\x86¤': 414, '`\\x8e\\x9e': 440, '^': 61, '\\x90\\x9f': 235, '\\x8b¥': 468, '\\x87£': 316, 'X\\x91\\x9e': 475, 'O\\x8b\\x9e': 349, 'Ä': 163, '\\x80\\x9e': 342, 'û': 218, 'X\\x8d\\x9e': 416, '\\x83\\x9f': 470, 'R\\x8d\\x9e': 363, '¹': 152, '\\x94¢': 379, 'P\\x8b\\x9e': 451, 'V\\x8f\\x9e': 343}\n"
          ]
        }
      ],
      "source": [
        "# Checking that the tokenizer \n",
        "vocab = tokenizer._bpe_model.get_vocab()\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9bCVISy7d_c",
        "outputId": "c59f1d95-0c4f-4fef-ed10-ce497d6d8ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ticks per beat: 384\n",
            "max tick: 730752\n",
            "tempo changes: 1\n",
            "time sig: 0\n",
            "key sig: 0\n",
            "markers: 0\n",
            "lyrics: False\n",
            "instruments: 1\n"
          ]
        }
      ],
      "source": [
        "# Testing to see that the tokenizer successfully reconverts back tokens to midi file\n",
        "import json\n",
        "with open('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE/MIDI-UNPROCESSED_14-15_R1_2014_MID--AUDIO_14_R1_2014_wav--1.json', 'r') as f:\n",
        "  data = json.load(f)['ids']\n",
        "midi = tokenizer.tokens_to_midi(data)\n",
        "print(midi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTdstWh-QKvH"
      },
      "source": [
        "Imports from Bachsformer - Codebase for GPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdE5HVO-pEMZ",
        "outputId": "6a760d57-78c2-44df-b6b2-408d689f7091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bachsformer'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 118 (delta 27), reused 95 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (118/118), 21.58 MiB | 8.67 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "/content/drive/Shareddrives/EC523_Project/CodeWorkspace/bachsformer/transformer_decoder_only\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pier-maker92/bachsformer.git\n",
        "%cd /content/drive/Shareddrives/EC523_Project/CodeWorkspace/bachsformer/transformer_decoder_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMHaab2u5hNi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from utils import CfgNode as CN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFvZRA4V6Qdr",
        "outputId": "4b0af9fc-dbe4-4fe6-a1d5-a752aa6a787f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwUCKWDv5kB-"
      },
      "outputs": [],
      "source": [
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        # these options must be filled in externally\n",
        "        C.vocab_size = None\n",
        "        C.block_size = None\n",
        "        # dropout hyperparameters\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        type_given = config.model_type is not None\n",
        "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        assert type_given ^ params_given # exactly one of these (XOR)\n",
        "        if type_given:\n",
        "            # translate from model_type to detailed configuration\n",
        "            config.merge_from_dict({\n",
        "                # names follow the huggingface naming conventions\n",
        "                # GPT-1\n",
        "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
        "                # GPT-2 configs\n",
        "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "                # Gophers\n",
        "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
        "                # (there are a number more...)\n",
        "                # I made these tiny models up\n",
        "                'gpt-mini':     dict(n_layer=12, n_head=12, n_embd=192),\n",
        "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
        "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
        "                'gpt-bach':     dict(n_layer=4, n_head=8, n_embd=128),\n",
        "            }[config.model_type])\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Initialize a pretrained GPT model by copying over the weights\n",
        "        from a huggingface/transformers checkpoint.\n",
        "        \"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 50257 # openai's model vocabulary\n",
        "        config.block_size = 1024  # openai's model block_size\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                # random note: because named_modules and named_parameters are recursive\n",
        "                # we will see the same tensors p many many times. but doing it this way\n",
        "                # allows us to know which parent module any tensor p belongs to...\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1Ad0LVE58yv"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZCQ23w555vf"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        # device to train on\n",
        "        C.device = 'auto'\n",
        "        # dataloder parameters\n",
        "        C.num_workers = 4\n",
        "        # optimizer parameters\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 64\n",
        "        C.learning_rate = 3e-4\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1 # only applied on matmul weights\n",
        "        C.grad_norm_clip = 1.0\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "\n",
        "        # determine the device we'll train on\n",
        "        if config.device == 'auto':\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        else:\n",
        "            self.device = config.device\n",
        "        self.model = self.model.to(self.device)\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "        # variables that will be assigned to trainer class later for logging and etc\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "    \n",
        "    def trainloader_setup(self,config):\n",
        "        self.train_dataset.shuffle_it()\n",
        "        # setup the dataloader\n",
        "        train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=self.train_dataset.__len__()),\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=config.num_workers,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "\n",
        "        train_loader = self.trainloader_setup(config)\n",
        "\n",
        "        # setup the optimizer\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "\n",
        "        \n",
        "        model.train()\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = time.time()\n",
        "        data_iter = iter(train_loader)\n",
        "        while True:\n",
        "\n",
        "            # fetch the next batch (x, y) and re-init iterator if needed\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                train_loader = self.trainloader_setup(config)\n",
        "                data_iter = iter(train_loader)\n",
        "                batch = next(data_iter)\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            x, y = batch\n",
        "\n",
        "            # forward the model\n",
        "            logits, self.loss = model(x, y)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.trigger_callbacks('on_batch_end')\n",
        "            self.iter_num += 1\n",
        "            tnow = time.time()\n",
        "            self.iter_dt = tnow - self.iter_time\n",
        "            self.iter_time = tnow\n",
        "\n",
        "            # termination conditions\n",
        "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4FYThtQEC8"
      },
      "source": [
        "Creating Dataset from Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E59qmDBjSmYQ"
      },
      "source": [
        "Process to convert json token ids into transformer input:\n",
        "\n",
        "1) For each json file, take 'ids', convert to list, drop remainder after dividing my block size (128)\n",
        "\n",
        "2) create Dataset Module where the x is taking [:-1] of data and y is taking [1:] from data, same as homework format.\n",
        "\n",
        "3) Input into Dataloader\n",
        "\n",
        "This approach is good for generation, not for sentiment analysis or translation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create dataset"
      ],
      "metadata": {
        "id": "ZbSzoEDZITPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9TPvqhUVUl-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "\n",
        "# Only for 1 json file, need to use in a loop and concatenate all the x and y outputs into 2 tensors\n",
        "class MakeDataset(Dataset):\n",
        "  def __init__(self, data, total_length, block_size=128):\n",
        "    # Load the JSON files from filepath\n",
        "    self.block_size = block_size\n",
        "    self.totlength = total_length\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return math.ceil(self.totlength / (self.block_size + 1))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
        "    chunk = self.data[i:i+self.block_size+1]\n",
        "    x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "    y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to break each song to block size then concatenate all tokens stored in json files in given folderpath"
      ],
      "metadata": {
        "id": "cPgtssUWIWWm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h4LLiEUS242"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def ConcatTokens(filepath, block_size):\n",
        "    totlength = 0;\n",
        "    ids=[]\n",
        "    json_files = [f for f in os.listdir(filepath) if f.endswith('.json')]\n",
        "    for file_name in json_files:\n",
        "      with open(os.path.join(filepath, file_name), 'r') as f:\n",
        "        data = json.load(f)['ids'][0]\n",
        "        data = data[:len(data)//(block_size+1)*(block_size+1)]\n",
        "        #print(len(data))\n",
        "        totlength = totlength + len(data)\n",
        "        ids += data\n",
        "\n",
        "    return ids, totlength"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the created dataset"
      ],
      "metadata": {
        "id": "ScpveF_gIs6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tCjq8fFzmmM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "#with open('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/train300.pkl', 'wb') as f:\n",
        "#  pickle.dump(tempx, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the created dataset"
      ],
      "metadata": {
        "id": "pIog2zpPIwzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo9tKgGLfwLc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Load train300 pickle file\n",
        "with open('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/trainfull.pkl', 'rb') as f:\n",
        "  dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to decide where the folder path for the tokens are and to concatenate all tokens by block size of model (192 for GPT-mini)"
      ],
      "metadata": {
        "id": "wSAgLgQaI2ty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyuFI1dSTpPz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "folder_path = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/train_tokenswithBPE_350'\n",
        "folder_path_full = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE_train'\n",
        "\n",
        "block_size = 192\n",
        "ids, totlength = ConcatTokens(folder_path_full, block_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aurm-r1dVBaZ",
        "outputId": "9668d88a-21d9-49bf-e1d5-67ca373f2ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14327741\n",
            "14327741\n"
          ]
        }
      ],
      "source": [
        "print(len(ids))\n",
        "print(totlength)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the dataset into a Dataset variable"
      ],
      "metadata": {
        "id": "-2iNEaYjI-gQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecOISoWfXbXN"
      },
      "outputs": [],
      "source": [
        "dataset = MakeDataset(ids, totlength, block_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX4zQGUMXxwY",
        "outputId": "aad02e82-ab50-439a-ed58-8cbc602b0c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74237\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix45mP4UhPqR",
        "outputId": "0ce71d67-6afd-48d2-9cfe-021d96049250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/EC523_Project/CodeWorkspace\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/Shareddrives/EC523_Project/CodeWorkspace\n",
        "import pickle\n",
        "#with open('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/trainfull.pkl', 'wb') as f:\n",
        "#  pickle.dump(dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq9uI8sjhYSy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Load train300 pickle file\n",
        "#with open('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/trainfull.pkl', 'rb') as f:\n",
        "#  dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurations and imports for the GPT model"
      ],
      "metadata": {
        "id": "N1xtgAulJNf4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IBhzas97j38"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfuOHBLkUtiX"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "# initialize a baby GPT model\n",
        "#model = classGPT(vocab_size =500, n_embd=128, n_head=4, block_size =128, n_layer=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4C-H_nXzzX2",
        "outputId": "040176dc-59bb-4db6-85bf-62f0e3d9afd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 5.47M\n"
          ]
        }
      ],
      "source": [
        "config = TrainerConfig(max_epochs=100, batch_size=64, learning_rate=6e-4,lr_decay=True, warmup_tokens=1024, final_tokens=150*len(dataset),num_workers=4)\n",
        "dtype = torch.float\n",
        "device = 'cpu'\n",
        "\n",
        "model_config = GPT.get_default_config()\n",
        "model_config.model_type = 'gpt-mini'\n",
        "model_config.vocab_size = 500\n",
        "model_config.block_size = 198\n",
        "\n",
        "model = GPT(model_config).to(device)\n",
        "model_name = \"bachsformer\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.cuda.current_device()\n",
        "  model.to(device)\n",
        "  optimizer = model.configure_optimizers(config)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2R9kI8V-fwD",
        "outputId": "e27bb337-1859-44bb-a36d-1d8673afb2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on device 0\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "steps_per_epoch = dataset.__len__()//batch_size\n",
        "train_config = Trainer.get_default_config()\n",
        "train_config.learning_rate = 6e-4 \n",
        "train_config.max_iters = steps_per_epoch*500\n",
        "train_config.num_workers = 0\n",
        "train_config.device=device\n",
        "train_config.batch_size = batch_size\n",
        "trainer = Trainer(train_config, model, dataset)\n",
        "try:\n",
        "  model.load_state_dict(torch.load(model_name))\n",
        "  print(\"model loaded from pretrained\")\n",
        "except: pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvvbqPh4BcAN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop for GPT model\n",
        "\n",
        "Saves the model with lowest loss during training"
      ],
      "metadata": {
        "id": "xPf3pcrjJTuH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feOoMrAIA3Hz",
        "outputId": "e3c659a1-af03-4b56-9698-e35df08b078f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1 iter 1159: train loss 3.49309. lr 1.087915e-04: 100%|██████████| 1160/1160 [08:18<00:00,  2.33it/s]\n",
            "epoch 2 iter 1159: train loss 3.21029. lr 2.436528e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 3 iter 1159: train loss 3.11786. lr 5.630105e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 4 iter 1159: train loss 3.02892. lr 6.000000e-05: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 5 iter 1159: train loss 2.85999. lr 3.922598e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 6 iter 1159: train loss 2.77482. lr 4.612366e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 7 iter 1159: train loss 2.71374. lr 6.000000e-05: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 8 iter 1159: train loss 2.63309. lr 5.181416e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 9 iter 1159: train loss 2.57080. lr 3.197471e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 10 iter 1159: train loss 2.51966. lr 6.000000e-05: 100%|██████████| 1160/1160 [08:16<00:00,  2.33it/s]\n",
            "epoch 11 iter 1159: train loss 2.54447. lr 5.902909e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.33it/s]\n",
            "epoch 12 iter 1159: train loss 2.44889. lr 1.733935e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 13 iter 1159: train loss 2.49907. lr 1.710409e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 14 iter 1159: train loss 2.44304. lr 5.909362e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 15 iter 1159: train loss 2.32932. lr 6.000000e-05: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 16 iter 1159: train loss 2.45718. lr 3.171517e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.33it/s]\n",
            "epoch 17 iter 1159: train loss 2.46342. lr 5.199184e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 18 iter 1159: train loss 2.34417. lr 6.000000e-05: 100%|██████████| 1160/1160 [08:15<00:00,  2.34it/s]\n",
            "epoch 19 iter 1159: train loss 2.48574. lr 4.590378e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 20 iter 1159: train loss 2.37639. lr 3.947306e-04: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 21 iter 1159: train loss 2.41514. lr 6.000000e-05: 100%|██████████| 1160/1160 [08:16<00:00,  2.34it/s]\n",
            "epoch 22 iter 1159: train loss 2.31613. lr 5.617498e-04: 100%|██████████| 1160/1160 [08:15<00:00,  2.34it/s]\n",
            "epoch 23 iter 1159: train loss 2.31560. lr 2.462089e-04: 100%|██████████| 1160/1160 [08:15<00:00,  2.34it/s]\n",
            "epoch 24 iter 1159: train loss 2.34346. lr 1.067950e-04: 100%|██████████| 1160/1160 [08:15<00:00,  2.34it/s]\n",
            "epoch 25 iter 1159: train loss 2.38627. lr 5.999880e-04: 100%|██████████| 1160/1160 [08:15<00:00,  2.34it/s]\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-31-a68248606223>\", line 61, in <cell line: 9>\n",
            "    torch.save(checkpoint, 'GPTminiepoch100.pt')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 440, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 315, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 288, in __init__\n",
            "    super().__init__(torch._C.PyTorchFileWriter(str(name)))\n",
            "RuntimeError: File GPTminiepoch100.pt cannot be opened.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-31-a68248606223>\", line 61, in <cell line: 9>\n",
            "    torch.save(checkpoint, 'GPTminiepoch100.pt')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 440, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 315, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 288, in __init__\n",
            "    super().__init__(torch._C.PyTorchFileWriter(str(name)))\n",
            "RuntimeError: File GPTminiepoch100.pt cannot be opened.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-31-a68248606223>\", line 61, in <cell line: 9>\n",
            "    torch.save(checkpoint, 'GPTminiepoch100.pt')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 440, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 315, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 288, in __init__\n",
            "    super().__init__(torch._C.PyTorchFileWriter(str(name)))\n",
            "RuntimeError: File GPTminiepoch100.pt cannot be opened.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(config)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "tokens = 0\n",
        "best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "\n",
        "\n",
        "for epoch in range(config.max_epochs):\n",
        "    model.train()\n",
        "    data = dataset \n",
        "    loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                        batch_size=train_config.batch_size,\n",
        "                        num_workers=train_config.num_workers)\n",
        "    losses = []\n",
        "    pbar = tqdm(enumerate(loader), total=len(loader)) \n",
        "    for iter, (x, y) in pbar:\n",
        "        # place data on the correct device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # forward the model\n",
        "        \"\"\" CODE HERE \"\"\"\n",
        "        outputs, _ = model(x)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), y.view(-1))\n",
        "        losses.append(loss.item())\n",
        "        # backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.grad_norm_clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        \"\"\" CODE HERE END \"\"\"\n",
        "        # decay the learning rate based on our progress\n",
        "        if config.lr_decay:\n",
        "            tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "            if tokens < config.warmup_tokens:\n",
        "                # linear warmup\n",
        "                lr_mult = float(tokens) / float(max(1, config.warmup_tokens))\n",
        "            else:\n",
        "                # cosine learning rate decay\n",
        "                progress = float(tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "            lr = config.learning_rate * lr_mult\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            lr = config.learning_rate\n",
        "        # report progress\n",
        "        pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "    avg_loss = sum(losses) / len(losses)\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        best_epoch = epoch\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'best_loss': best_loss,\n",
        "            'best_epoch': best_epoch\n",
        "        }\n",
        "        torch.save(checkpoint, 'GPTminiepoch100.pt')\n",
        "        \n",
        "    # save the epoch and training loss information separately\n",
        "    loss_info = {'epoch': epoch, 'train_loss': avg_loss}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved model"
      ],
      "metadata": {
        "id": "wK-rVa-PJcZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxs600Fl1x8N",
        "outputId": "68036f47-8ff3-4e7e-e26e-f9ffb42b0c55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "checkpoint = torch.load('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/bachsformer/transformer_decoder_only/GPTminiepoch100.pt')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e-B8lu-2CeK"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/bachsformer/transformer_decoder_only/GPTminiepoch100.pt')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['best_loss']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to generate and sample the generated music"
      ],
      "metadata": {
        "id": "xPk6qWaYJgXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or6f3VEnq9SV"
      },
      "outputs": [],
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. \n",
        "    \"\"\"\n",
        "    block_size = dataset.block_size\n",
        "    print(block_size)\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grab the test data and concatenate the tokens"
      ],
      "metadata": {
        "id": "DOJPWDWhJmAX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgn2WfxfuDwx",
        "outputId": "5ad74175-d1db-447f-f725-bfdbe07cecf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1851256\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "folder_path = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE_test'\n",
        "\n",
        "block_size = 192\n",
        "ids, totlength = ConcatTokens(folder_path, block_size)\n",
        "print(len(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to generate 1 random song"
      ],
      "metadata": {
        "id": "lhQvty8HJs8I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0W1LEO_uoos",
        "outputId": "cd74e568-6b6f-4b2f-97d1-d28da2990936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1760516\n",
            "tensor([193,  60, 115, 131, 198,  49, 254, 200,  50, 247, 202,  53, 270, 204,\n",
            "         59, 251, 209,  30, 280,  40, 282, 210,  57, 273,  62, 309, 211,  50,\n",
            "        255, 217,  44, 241, 220, 466,  60, 325, 257,  48, 241, 193,  52, 280,\n",
            "        195,  39, 294, 196,  30, 268, 197,  51, 105, 135,  59, 110, 141, 203,\n",
            "         47, 239, 205,  48, 253, 208,  52, 265, 212,  57, 262, 217,  31, 102,\n",
            "        152, 218,  41,  97, 133, 219,  47, 101, 140,  57, 107, 138, 220,  50,\n",
            "        101, 147, 300,  37, 312, 198,  38, 297, 202,  40, 110, 132, 208,  41,\n",
            "        101, 133, 209,  56, 107, 142, 215,  53, 102, 133, 220,  50, 101, 132,\n",
            "        397,  47, 271, 203,  19,  98, 134, 206,  40, 289, 209,  48, 328, 214,\n",
            "         55, 105, 135, 220,  43, 289, 274,  50, 256, 196,  48, 301, 200,  31,\n",
            "        101, 151,  43,  99, 140, 201,  60, 107, 136, 206,  48, 239, 210,  53,\n",
            "        103, 136, 214,  42, 339,  52, 246, 220,  41, 103, 147, 234,  50, 284,\n",
            "         60, 107, 134, 194,  49, 268, 198,  52, 106, 138, 202,  50, 468, 206,\n",
            "         55, 311,  59, 499, 210,  57, 106, 135, 211,  42], device='cuda:0')\n",
            "128\n",
            "[193, 60, 115, 131, 198, 49, 254, 200, 50, 247, 202, 53, 270, 204, 59, 251, 209, 30, 280, 40, 282, 210, 57, 273, 62, 309, 211, 50, 255, 217, 44, 241, 220, 466, 60, 325, 257, 48, 241, 193, 52, 280, 195, 39, 294, 196, 30, 268, 197, 51, 105, 135, 59, 110, 141, 203, 47, 239, 205, 48, 253, 208, 52, 265, 212, 57, 262, 217, 31, 102, 152, 218, 41, 97, 133, 219, 47, 101, 140, 57, 107, 138, 220, 50, 101, 147, 300, 37, 312, 198, 38, 297, 202, 40, 110, 132, 208, 41, 101, 133, 209, 56, 107, 142, 215, 53, 102, 133, 220, 50, 101, 132, 397, 47, 271, 203, 19, 98, 134, 206, 40, 289, 209, 48, 328, 214, 55, 105, 135, 220, 43, 289, 274, 50, 256, 196, 48, 301, 200, 31, 101, 151, 43, 99, 140, 201, 60, 107, 136, 206, 48, 239, 210, 53, 103, 136, 214, 42, 339, 52, 246, 220, 41, 103, 147, 234, 50, 284, 60, 107, 134, 194, 49, 268, 198, 52, 106, 138, 202, 50, 468, 206, 55, 311, 59, 499, 210, 57, 106, 135, 211, 42, 102, 134, 215, 48, 232, 220, 31, 101, 133, 47, 104, 136, 274, 38, 102, 135, 196, 43, 103, 135, 200, 55, 106, 139, 201, 53, 102, 137, 204, 50, 103, 135, 208, 43, 102, 137, 212, 47, 103, 133, 217, 48, 101, 133, 52, 102, 133, 244, 43, 101, 132, 195, 47, 101, 132, 50, 101, 132, 200, 43, 100, 132, 205, 48, 101, 134, 52, 101, 135, 211, 43, 100, 131, 215, 45, 100, 132, 50, 101, 133, 220, 43, 100, 132, 300, 48, 101, 133, 52, 100, 133, 198, 43, 99, 133, 203, 47, 98, 132, 50, 100, 132, 207, 43, 97, 133, 212, 48, 99, 132, 52, 101, 132, 216, 43, 98, 131, 234, 47, 101, 132, 50, 100, 132, 194, 43, 99, 132, 199, 48, 98, 132, 52, 101, 132, 204, 43, 99, 133, 210, 45, 100, 132, 50, 100, 132, 215, 43, 98, 132, 244, 48, 99, 132, 52, 100, 132, 196, 43, 98, 132, 202, 47, 100, 133, 50, 100, 132, 207, 43, 100, 132, 213, 48, 100, 132, 52, 101, 132, 218, 43, 99, 132, 274, 47, 100, 132, 50, 100, 132, 197, 43, 99, 132, 202, 45, 100, 132, 50, 101, 132, 207, 43, 459, 214, 48, 101, 132, 52, 100, 131, 219, 43, 346, 300, 47, 101, 132, 50, 459, 198, 43, 102, 133, 205, 48, 459, 52, 101, 132, 210, 43, 102, 133, 216, 47, 101, 132, 50, 100, 132, 234, 43, 101, 132, 195, 45, 101, 132, 48, 100, 132, 201, 43, 101, 132, 207, 42, 101, 132, 50, 101, 132, 213, 43, 100, 132, 219, 45, 99, 132, 48, 346, 274, 43, 101, 132, 198, 42, 100, 132, 50, 100, 132, 205, 43, 100, 132, 211, 48, 101, 132, 52, 418, 217, 43, 100, 132, 244, 47, 100, 132, 50, 334, 195, 43, 414, 201, 45, 346, 48, 418, 206, 43, 334, 211, 42, 334, 50, 339, 216, 43, 103, 133, 234, 45, 101, 132, 48, 428, 194, 43, 102, 133, 201, 42, 100, 132, 50, 103, 133, 207, 45, 103, 133, 214, 43, 102, 133, 50, 442, 220, 45, 101, 132, 48, 103, 133, 323, 43, 102, 133, 200, 42, 101, 133, 50, 103, 133, 206, 45, 102, 133, 48, 103, 133, 212, 43, 104, 133, 217, 45, 104, 133, 48, 107, 133, 244, 43, 106, 134, 196, 42, 105, 133, 50, 109, 133, 202, 43, 468, 208, 45, 428, 48, 488, 213, 42, 106, 133, 218, 43, 107, 133, 257, 45, 468, 48, 499, 196, 43, 106, 133, 201, 42, 103, 134, 50, 109, 133, 206, 45, 428, 48, 108, 133, 211, 43, 105, 133, 216, 42, 105, 134, 50, 109, 133, 234, 45, 488, 48, 111, 133, 194, 43, 106, 133, 199, 42, 103, 133, 50, 110, 134, 204, 45, 107, 133, 48, 112, 132, 209, 43, 106, 133, 52, 111, 133, 215, 45, 468, 48, 109, 132, 220, 42, 109, 133, 300, 43, 106, 133, 50, 112, 132, 198, 45, 109, 132, 48, 112, 132, 203, 43, 108, 133, 52, 112, 133, 208, 42, 499, 50, 113, 132, 213, 45, 110, 133, 48, 455, 218, 43, 419, 52, 111, 132, 257, 42, 110, 132, 50, 113, 132, 196, 45, 111, 132, 48, 112, 132, 201, 43, 111, 132, 52, 113, 132, 206, 42, 111, 132, 50, 113, 132, 211, 45, 109, 132, 48, 113, 132, 216, 43, 110, 132, 52, 113, 132, 234, 45, 111, 133, 48, 113, 132, 194, 43, 111, 132, 52, 112, 132, 199, 42, 109, 132, 50, 112, 132, 204, 45, 112, 132, 48, 114, 132, 209, 43, 110, 132, 52, 113, 132, 214, 42, 109, 132, 50, 112, 132, 219, 45, 111, 132, 48, 113, 132, 300, 43, 109, 132, 52, 114, 131, 198, 42, 499, 50, 113, 132, 203, 45, 110, 132, 48, 113, 132, 208, 43, 111, 133, 52, 114, 133, 213, 42, 109, 132, 50, 113, 132, 218, 45, 109, 132, 48, 112, 132, 257, 43, 110, 132, 52, 114, 132, 196, 42, 109, 132, 50, 113, 132, 201, 45, 109, 132, 48, 112, 133, 206, 43, 110, 133, 52, 114, 132, 211, 42, 395, 50, 113, 132, 215, 48, 111, 132, 216, 43, 370, 220, 47, 112, 132, 234, 38, 499, 194, 45, 109, 132, 48, 110, 132, 200, 42, 106, 133, 50, 110, 133, 205, 40, 105, 133, 48, 108, 133, 210, 50, 111, 133, 211, 38, 105, 134, 216, 36, 428, 52, 111, 133, 234, 35, 442, 50, 111, 132, 194, 33, 105, 133, 48, 109, 133, 199, 31, 104, 133, 47, 108, 134, 204, 48, 109, 132, 205, 33, 418, 210, 35, 103, 135, 50, 108, 133, 215, 52, 109, 133, 216, 31, 103, 133, 234, 53, 109, 132, 190, 33, 428, 195, 35, 103, 133, 55, 108, 133, 201, 38, 103, 134, 53, 488, 206, 36, 334, 52, 108, 133, 212, 38, 104, 133, 53, 109, 133, 218, 35, 103, 133, 55, 108, 133, 274, 36, 103, 133, 52, 107, 133, 198, 38, 103, 134, 53, 107, 133, 204, 55, 109, 132, 205, 40, 104, 133, 210, 41, 468, 57, 110, 133, 216, 38, 468, 53, 109, 133, 244, 40, 108, 133, 55, 108, 133, 197, 41, 105, 133, 57, 110, 133, 204, 38, 107, 133, 53, 109, 133, 210, 36, 106, 133, 52, 109, 134, 216, 35, 106, 133, 50, 110, 133, 244, 36, 106, 133, 52, 111, 133, 196, 38, 107, 133, 53, 111, 132, 202, 35, 106, 133, 55, 111, 132, 209, 36, 108, 133, 52, 111, 134, 216, 38, 108, 133, 53, 110, 132, 257, 36, 108, 133, 52, 111, 133, 197, 35, 109, 134, 50, 111, 132, 204, 36, 109, 132, 52, 112, 132, 210, 35, 111, 132, 50, 112, 132, 216, 33, 110, 132, 48, 112, 132, 257, 35, 111, 132, 50, 111, 132, 197, 36, 109, 132, 52, 112, 132, 203, 35, 109, 133, 50, 111, 133, 209, 33, 109, 132, 48, 111, 132, 216, 31, 109, 132, 47, 110, 132, 257, 33, 109, 133, 48, 109, 132, 198, 35, 108, 133, 50, 110, 132, 204, 48, 110, 132, 205, 36, 109, 132, 211, 35, 107, 133, 50, 110, 133, 217, 33, 499, 48, 110, 132, 274, 35, 107, 134, 50, 110, 133, 199, 36, 109, 133, 52, 109, 133, 205, 35, 109, 133, 50, 109, 133, 211, 36, 109, 132, 52, 419, 217, 35, 109, 132, 50, 111, 133, 244, 36, 110, 132, 52, 111, 133, 197, 35, 110, 132, 50, 110, 132, 203, 33, 109, 133, 48, 112, 133, 210, 35, 112, 133, 50, 113, 133, 216, 33, 111, 132, 48, 112, 132, 244, 35, 113, 133, 50, 111, 132, 196, 36, 111, 132, 52, 112, 133, 202, 35, 112, 132, 50, 113, 132, 208, 33, 112, 132, 48, 113, 132, 214, 31, 111, 132, 47, 112, 132, 244, 33, 112, 132, 48, 111, 134, 198, 31, 112, 133, 47, 114, 132, 205, 33, 113, 132, 48, 110, 132, 212, 31, 110, 133, 43, 113, 132, 218, 42, 113, 133, 219, 26, 110, 133, 397, 31, 110, 132, 46, 113, 132, 201, 33, 111, 133, 48, 113, 132, 207, 31, 112, 132, 46, 112, 133, 215, 29, 109, 133, 45, 112, 134, 234, 31, 110, 133, 48, 112, 133, 196, 33, 110, 132, 48, 113, 132, 202, 31, 111, 133, 46, 112, 132, 209, 33, 111, 132, 48, 113, 132, 216, 34, 112, 132, 50, 114, 132, 244, 36, 113, 131, 51, 113, 132, 197, 34, 114, 131, 50, 115, 131, 204, 36, 114, 132, 51, 114, 132, 211, 38, 114, 132, 53, 113, 132, 218, 39, 113, 131, 55, 113, 132, 300, 41, 114, 131, 50, 113, 131, 200, 39, 112, 132, 48, 112, 132, 55, 112, 132, 207, 38, 113, 131, 53, 113, 131, 214, 36, 113, 131, 51, 112, 132, 234, 34, 476, 55, 113, 131, 195, 32, 112, 132, 53, 113, 131, 202, 31, 112, 132, 51, 112, 132, 209, 29, 110, 134, 50, 350, 216, 31, 111, 132, 48, 111, 132, 257, 32, 111, 132, 50, 111, 132, 198, 34, 111, 132, 199, 48, 476, 206, 32, 112, 132, 50, 113, 131, 213, 31, 111, 133, 51, 113, 133, 219, 29, 112, 132, 48, 113, 132, 323, 27, 111, 132, 55, 112, 132, 201, 29, 112, 132, 56, 113, 132, 207, 31, 113, 132, 58, 113, 132, 213, 32, 113, 132, 60, 113, 132, 219, 34, 114, 132, 220, 62, 447, 323, 36, 114, 131, 63, 114, 132, 201, 38, 114, 131, 65, 114, 131, 208, 39, 114, 132, 67, 113, 132, 215, 41, 112, 132, 65, 112, 133, 244, 43, 112, 133, 63, 112, 132, 197, 41, 110, 133, 62, 111, 132, 204, 43, 110, 132, 63, 112, 133, 212, 44, 109, 132, 65, 112, 132, 219, 43, 109, 132, 63, 111, 132, 300, 41, 108, 133, 62, 111, 133, 201, 39, 108, 133, 60, 111, 132, 208, 38, 108, 133, 65, 111, 133, 216, 36, 108, 133, 63, 112, 132, 257, 38, 110, 132, 192, 65, 112, 132, 199, 39, 110, 132, 67, 111, 132, 207, 41, 109, 132, 68, 111, 132, 213, 39, 499, 67, 111, 132, 219, 41, 499, 65, 110, 132, 323, 43, 106, 133, 63, 109, 132, 201, 41, 442, 62, 370, 207, 43, 339, 63, 107, 133, 214, 44, 104, 133, 65, 109, 133, 220, 46, 442, 63, 107, 133, 323, 48, 468, 62, 488, 202, 46, 442, 63, 499, 210, 44, 105, 133, 62, 488, 216, 60, 499, 257, 43, 105, 133, 58, 106, 133, 198, 41, 418, 56, 468, 204, 43, 339, 58, 488, 210, 44, 351, 60, 488, 217, 46, 488, 58, 488, 257, 48, 370, 56, 499, 198, 46, 442, 55, 108, 133, 205, 53, 468, 206, 44, 418, 212, 43, 428, 51, 468, 218, 41, 428, 50, 106, 133, 300, 39, 103, 133, 51, 106, 134, 201, 38, 428, 53, 468, 207, 39, 428, 55, 488, 214, 41, 104, 133, 56, 107, 133, 244, 39, 418, 55, 107, 133, 198, 32, 102, 135, 53, 105, 133, 206, 34, 103, 134, 51, 105, 134, 214, 31, 428, 55, 488, 220, 53, 107, 133, 234, 32, 339, 195, 34, 418, 51, 468, 203, 32, 334, 50, 362, 210, 51, 107, 133, 211, 34, 339, 217, 53, 107, 133, 218, 31, 442, 300, 32, 103, 134, 50, 108, 133, 200, 34, 334, 51, 106, 133, 207, 36, 418, 53, 107, 133, 215, 34, 428, 55, 109, 132, 244, 32, 105, 133, 53, 488, 198, 31, 442, 51, 499, 205, 32, 468, 50, 109, 132, 213, 31, 468, 51, 109, 132, 234, 29, 488, 48, 109, 132, 199, 31, 107, 133, 46, 110, 132, 207, 32, 488, 48, 110, 132, 215, 34, 370, 50, 110, 132, 234, 36, 109, 132, 51, 111, 132, 196, 53, 455, 197, 38, 378, 204, 39, 468, 55, 113, 132, 211, 41, 488, 56, 476, 218, 39, 370, 55, 455, 323, 41, 378, 56, 113, 131, 200, 39, 109, 132, 55, 113, 132, 206, 38, 378, 53, 113, 131, 213, 36, 109, 132, 51, 113, 132, 220, 34, 109, 132, 50, 476, 397, 36, 109, 132, 51, 113, 131, 202, 38, 111, 132, 53, 112, 132, 209, 39, 109, 132, 55, 112, 132, 217, 41, 109, 132, 48, 455, 57, 112, 132, 300, 43, 109, 132, 50, 109, 132, 58, 111, 132, 200, 41, 108, 133, 48, 109, 132, 57, 112, 132, 207, 43, 109, 132, 58, 112, 132, 213, 41, 108, 133, 51, 111, 132, 60, 112, 132, 219, 39, 109, 132, 220, 55, 111, 132, 323, 38, 109, 132, 53, 110, 132, 201, 36, 488, 51, 112, 132, 208, 34, 108, 133, 50, 113, 131, 216, 33, 108, 133, 48, 112, 133, 274, 34, 109, 132, 50, 111, 133, 199, 36, 108, 133, 51, 112, 132, 206, 34, 109, 132, 50, 111, 132, 213, 33, 109, 132, 48, 111, 133, 219, 34, 109, 133, 50, 112, 133, 397, 36, 110, 132, 51, 112, 132, 201, 38, 109, 132, 53, 111, 132]\n",
            "[102, 134, 215, 48, 232, 220, 31, 101, 133, 47, 104, 136, 274, 38, 102, 135, 196, 43, 103, 135, 200, 55, 106, 139, 201, 53, 102, 137, 204, 50, 103, 135, 208, 43, 102, 137, 212, 47, 103, 133, 217, 48, 101, 133, 52, 102, 133, 244, 43, 101, 132, 195, 47, 101, 132, 50, 101, 132, 200, 43, 100, 132, 205, 48, 101, 134, 52, 101, 135, 211, 43, 100, 131, 215, 45, 100, 132, 50, 101, 133, 220, 43, 100, 132, 300, 48, 101, 133, 52, 100, 133, 198, 43, 99, 133, 203, 47, 98, 132, 50, 100, 132, 207, 43, 97, 133, 212, 48, 99, 132, 52, 101, 132, 216, 43, 98, 131, 234, 47, 101, 132, 50, 100, 132, 194, 43, 99, 132, 199, 48, 98, 132, 52, 101, 132, 204, 43, 99, 133, 210, 45, 100, 132, 50, 100, 132, 215, 43, 98, 132, 244, 48, 99, 132, 52, 100, 132, 196, 43, 98, 132, 202, 47, 100, 133, 50, 100, 132, 207, 43, 100, 132, 213, 48, 100, 132, 52, 101, 132, 218, 43, 99, 132, 274, 47, 100, 132, 50, 100, 132, 197, 43, 99, 132, 202, 45, 100, 132, 50, 101, 132, 207, 43, 459, 214, 48, 101, 132, 52, 100, 131, 219, 43, 346, 300, 47, 101, 132, 50, 459, 198, 43, 102, 133, 205, 48, 459, 52, 101, 132, 210, 43, 102, 133, 216, 47, 101, 132, 50, 100, 132, 234, 43, 101, 132, 195, 45, 101, 132, 48, 100, 132, 201, 43, 101, 132, 207, 42, 101, 132, 50, 101, 132, 213, 43, 100, 132, 219, 45, 99, 132, 48, 346, 274, 43, 101, 132, 198, 42, 100, 132, 50, 100, 132, 205, 43, 100, 132, 211, 48, 101, 132, 52, 418, 217, 43, 100, 132, 244, 47, 100, 132, 50, 334, 195, 43, 414, 201, 45, 346, 48, 418, 206, 43, 334, 211, 42, 334, 50, 339, 216, 43, 103, 133, 234, 45, 101, 132, 48, 428, 194, 43, 102, 133, 201, 42, 100, 132, 50, 103, 133, 207, 45, 103, 133, 214, 43, 102, 133, 50, 442, 220, 45, 101, 132, 48, 103, 133, 323, 43, 102, 133, 200, 42, 101, 133, 50, 103, 133, 206, 45, 102, 133, 48, 103, 133, 212, 43, 104, 133, 217, 45, 104, 133, 48, 107, 133, 244, 43, 106, 134, 196, 42, 105, 133, 50, 109, 133, 202, 43, 468, 208, 45, 428, 48, 488, 213, 42, 106, 133, 218, 43, 107, 133, 257, 45, 468, 48, 499, 196, 43, 106, 133, 201, 42, 103, 134, 50, 109, 133, 206, 45, 428, 48, 108, 133, 211, 43, 105, 133, 216, 42, 105, 134, 50, 109, 133, 234, 45, 488, 48, 111, 133, 194, 43, 106, 133, 199, 42, 103, 133, 50, 110, 134, 204, 45, 107, 133, 48, 112, 132, 209, 43, 106, 133, 52, 111, 133, 215, 45, 468, 48, 109, 132, 220, 42, 109, 133, 300, 43, 106, 133, 50, 112, 132, 198, 45, 109, 132, 48, 112, 132, 203, 43, 108, 133, 52, 112, 133, 208, 42, 499, 50, 113, 132, 213, 45, 110, 133, 48, 455, 218, 43, 419, 52, 111, 132, 257, 42, 110, 132, 50, 113, 132, 196, 45, 111, 132, 48, 112, 132, 201, 43, 111, 132, 52, 113, 132, 206, 42, 111, 132, 50, 113, 132, 211, 45, 109, 132, 48, 113, 132, 216, 43, 110, 132, 52, 113, 132, 234, 45, 111, 133, 48, 113, 132, 194, 43, 111, 132, 52, 112, 132, 199, 42, 109, 132, 50, 112, 132, 204, 45, 112, 132, 48, 114, 132, 209, 43, 110, 132, 52, 113, 132, 214, 42, 109, 132, 50, 112, 132, 219, 45, 111, 132, 48, 113, 132, 300, 43, 109, 132, 52, 114, 131, 198, 42, 499, 50, 113, 132, 203, 45, 110, 132, 48, 113, 132, 208, 43, 111, 133, 52, 114, 133, 213, 42, 109, 132, 50, 113, 132, 218, 45, 109, 132, 48, 112, 132, 257, 43, 110, 132, 52, 114, 132, 196, 42, 109, 132, 50, 113, 132, 201, 45, 109, 132, 48, 112, 133, 206, 43, 110, 133, 52, 114, 132, 211, 42, 395, 50, 113, 132, 215, 48, 111, 132, 216, 43, 370, 220, 47, 112, 132, 234, 38, 499, 194, 45, 109, 132, 48, 110, 132, 200, 42, 106, 133, 50, 110, 133, 205, 40, 105, 133, 48, 108, 133, 210, 50, 111, 133, 211, 38, 105, 134, 216, 36, 428, 52, 111, 133, 234, 35, 442, 50, 111, 132, 194, 33, 105, 133, 48, 109, 133, 199, 31, 104, 133, 47, 108, 134, 204, 48, 109, 132, 205, 33, 418, 210, 35, 103, 135, 50, 108, 133, 215, 52, 109, 133, 216, 31, 103, 133, 234, 53, 109, 132, 190, 33, 428, 195, 35, 103, 133, 55, 108, 133, 201, 38, 103, 134, 53, 488, 206, 36, 334, 52, 108, 133, 212, 38, 104, 133, 53, 109, 133, 218, 35, 103, 133, 55, 108, 133, 274, 36, 103, 133, 52, 107, 133, 198, 38, 103, 134, 53, 107, 133, 204, 55, 109, 132, 205, 40, 104, 133, 210, 41, 468, 57, 110, 133, 216, 38, 468, 53, 109, 133, 244, 40, 108, 133, 55, 108, 133, 197, 41, 105, 133, 57, 110, 133, 204, 38, 107, 133, 53, 109, 133, 210, 36, 106, 133, 52, 109, 134, 216, 35, 106, 133, 50, 110, 133, 244, 36, 106, 133, 52, 111, 133, 196, 38, 107, 133, 53, 111, 132, 202, 35, 106, 133, 55, 111, 132, 209, 36, 108, 133, 52, 111, 134, 216, 38, 108, 133, 53, 110, 132, 257, 36, 108, 133, 52, 111, 133, 197, 35, 109, 134, 50, 111, 132, 204, 36, 109, 132, 52, 112, 132, 210, 35, 111, 132, 50, 112, 132, 216, 33, 110, 132, 48, 112, 132, 257, 35, 111, 132, 50, 111, 132, 197, 36, 109, 132, 52, 112, 132, 203, 35, 109, 133, 50, 111, 133, 209, 33, 109, 132, 48, 111, 132, 216, 31, 109, 132, 47, 110, 132, 257, 33, 109, 133, 48, 109, 132, 198, 35, 108, 133, 50, 110, 132, 204, 48, 110, 132, 205, 36, 109, 132, 211, 35, 107, 133, 50, 110, 133, 217, 33, 499, 48, 110, 132, 274, 35, 107, 134, 50, 110, 133, 199, 36, 109, 133, 52, 109, 133, 205, 35, 109, 133, 50, 109, 133, 211, 36, 109, 132, 52, 419, 217, 35, 109, 132, 50, 111, 133, 244, 36, 110, 132, 52, 111, 133, 197, 35, 110, 132, 50, 110, 132, 203, 33, 109, 133, 48, 112, 133, 210, 35, 112, 133, 50, 113, 133, 216, 33, 111, 132, 48, 112, 132, 244, 35, 113, 133, 50, 111, 132, 196, 36, 111, 132, 52, 112, 133, 202, 35, 112, 132, 50, 113, 132, 208, 33, 112, 132, 48, 113, 132, 214, 31, 111, 132, 47, 112, 132, 244, 33, 112, 132, 48, 111, 134, 198, 31, 112, 133, 47, 114, 132, 205, 33, 113, 132, 48, 110, 132, 212, 31, 110, 133, 43, 113, 132, 218, 42, 113, 133, 219, 26, 110, 133, 397, 31, 110, 132, 46, 113, 132, 201, 33, 111, 133, 48, 113, 132, 207, 31, 112, 132, 46, 112, 133, 215, 29, 109, 133, 45, 112, 134, 234, 31, 110, 133, 48, 112, 133, 196, 33, 110, 132, 48, 113, 132, 202, 31, 111, 133, 46, 112, 132, 209, 33, 111, 132, 48, 113, 132, 216, 34, 112, 132, 50, 114, 132, 244, 36, 113, 131, 51, 113, 132, 197, 34, 114, 131, 50, 115, 131, 204, 36, 114, 132, 51, 114, 132, 211, 38, 114, 132, 53, 113, 132, 218, 39, 113, 131, 55, 113, 132, 300, 41, 114, 131, 50, 113, 131, 200, 39, 112, 132, 48, 112, 132, 55, 112, 132, 207, 38, 113, 131, 53, 113, 131, 214, 36, 113, 131, 51, 112, 132, 234, 34, 476, 55, 113, 131, 195, 32, 112, 132, 53, 113, 131, 202, 31, 112, 132, 51, 112, 132, 209, 29, 110, 134, 50, 350, 216, 31, 111, 132, 48, 111, 132, 257, 32, 111, 132, 50, 111, 132, 198, 34, 111, 132, 199, 48, 476, 206, 32, 112, 132, 50, 113, 131, 213, 31, 111, 133, 51, 113, 133, 219, 29, 112, 132, 48, 113, 132, 323, 27, 111, 132, 55, 112, 132, 201, 29, 112, 132, 56, 113, 132, 207, 31, 113, 132, 58, 113, 132, 213, 32, 113, 132, 60, 113, 132, 219, 34, 114, 132, 220, 62, 447, 323, 36, 114, 131, 63, 114, 132, 201, 38, 114, 131, 65, 114, 131, 208, 39, 114, 132, 67, 113, 132, 215, 41, 112, 132, 65, 112, 133, 244, 43, 112, 133, 63, 112, 132, 197, 41, 110, 133, 62, 111, 132, 204, 43, 110, 132, 63, 112, 133, 212, 44, 109, 132, 65, 112, 132, 219, 43, 109, 132, 63, 111, 132, 300, 41, 108, 133, 62, 111, 133, 201, 39, 108, 133, 60, 111, 132, 208, 38, 108, 133, 65, 111, 133, 216, 36, 108, 133, 63, 112, 132, 257, 38, 110, 132, 192, 65, 112, 132, 199, 39, 110, 132, 67, 111, 132, 207, 41, 109, 132, 68, 111, 132, 213, 39, 499, 67, 111, 132, 219, 41, 499, 65, 110, 132, 323, 43, 106, 133, 63, 109, 132, 201, 41, 442, 62, 370, 207, 43, 339, 63, 107, 133, 214, 44, 104, 133, 65, 109, 133, 220, 46, 442, 63, 107, 133, 323, 48, 468, 62, 488, 202, 46, 442, 63, 499, 210, 44, 105, 133, 62, 488, 216, 60, 499, 257, 43, 105, 133, 58, 106, 133, 198, 41, 418, 56, 468, 204, 43, 339, 58, 488, 210, 44, 351, 60, 488, 217, 46, 488, 58, 488, 257, 48, 370, 56, 499, 198, 46, 442, 55, 108, 133, 205, 53, 468, 206, 44, 418, 212, 43, 428, 51, 468, 218, 41, 428, 50, 106, 133, 300, 39, 103, 133, 51, 106, 134, 201, 38, 428, 53, 468, 207, 39, 428, 55, 488, 214, 41, 104, 133, 56, 107, 133, 244, 39, 418, 55, 107, 133, 198, 32, 102, 135, 53, 105, 133, 206, 34, 103, 134, 51, 105, 134, 214, 31, 428, 55, 488, 220, 53, 107, 133, 234, 32, 339, 195, 34, 418, 51, 468, 203, 32, 334, 50, 362, 210, 51, 107, 133, 211, 34, 339, 217, 53, 107, 133, 218, 31, 442, 300, 32, 103, 134, 50, 108, 133, 200, 34, 334, 51, 106, 133, 207, 36, 418, 53, 107, 133, 215, 34, 428, 55, 109, 132, 244, 32, 105, 133, 53, 488, 198, 31, 442, 51, 499, 205, 32, 468, 50, 109, 132, 213, 31, 468, 51, 109, 132, 234, 29, 488, 48, 109, 132, 199, 31, 107, 133, 46, 110, 132, 207, 32, 488, 48, 110, 132, 215, 34, 370, 50, 110, 132, 234, 36, 109, 132, 51, 111, 132, 196, 53, 455, 197, 38, 378, 204, 39, 468, 55, 113, 132, 211, 41, 488, 56, 476, 218, 39, 370, 55, 455, 323, 41, 378, 56, 113, 131, 200, 39, 109, 132, 55, 113, 132, 206, 38, 378, 53, 113, 131, 213, 36, 109, 132, 51, 113, 132, 220, 34, 109, 132, 50, 476, 397, 36, 109, 132, 51, 113, 131, 202, 38, 111, 132, 53, 112, 132, 209, 39, 109, 132, 55, 112, 132, 217, 41, 109, 132, 48, 455, 57, 112, 132, 300, 43, 109, 132, 50, 109, 132, 58, 111, 132, 200, 41, 108, 133, 48, 109, 132, 57, 112, 132, 207, 43, 109, 132, 58, 112, 132, 213, 41, 108, 133, 51, 111, 132, 60, 112, 132, 219, 39, 109, 132, 220, 55, 111, 132, 323, 38, 109, 132, 53, 110, 132, 201, 36, 488, 51, 112, 132, 208, 34, 108, 133, 50, 113, 131, 216, 33, 108, 133, 48, 112, 133, 274, 34, 109, 132, 50, 111, 133, 199, 36, 108, 133, 51, 112, 132, 206, 34, 109, 132, 50, 111, 132, 213, 33, 109, 132, 48, 111, 133, 219, 34, 109, 133, 50, 112, 133, 397, 36, 110, 132, 51, 112, 132, 201, 38, 109, 132, 53, 111, 132]\n",
            "ticks per beat: 384\n",
            "max tick: 94656\n",
            "tempo changes: 1\n",
            "time sig: 0\n",
            "key sig: 0\n",
            "markers: 0\n",
            "lyrics: False\n",
            "instruments: 1\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random_start = random.randint(0, len(ids)-block_size)\n",
        "print(random_start)\n",
        "testtestids = torch.tensor(ids[random_start:random_start+block_size], dtype=torch.long).to(device)\n",
        "print(testtestids)\n",
        "x = testtestids.unsqueeze(0)\n",
        "y = sample(model, x, 2000, temperature=0.5, sample=True, top_k=10)[0]\n",
        "print(y.tolist())\n",
        "result = y.tolist()\n",
        "result = result[192:]\n",
        "print(result)\n",
        "midi = tokenizer.tokens_to_midi([result])\n",
        "print(midi)\n",
        "midi.dump('/content/drive/Shareddrives/EC523_Project/CodeWorkspace/result/GPTmini/file.mid')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to generate multiple music and store with iterative names"
      ],
      "metadata": {
        "id": "F1sDlWm6J3ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generatemusic(model, data, block_size, emblength, temperature, top_k, folderpath, numbergen):\n",
        "  for i in range(numbergen):\n",
        "    random_start = random.randint(0, len(data)-block_size)\n",
        "    testtestids = torch.tensor(data[random_start:random_start+block_size], dtype=torch.long).to(device)\n",
        "    x = testtestids.unsqueeze(0)\n",
        "    y = sample(model, x, emblength, temperature, sample=True, top_k=top_k)[0]\n",
        "    result = y.tolist()\n",
        "    result = result[192:]\n",
        "    midi = tokenizer.tokens_to_midi([result])\n",
        "    midi.dump(folderpath+f'{i}.mid')"
      ],
      "metadata": {
        "id": "_-_H7ksOuO-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "folder_path = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/result/GPTmini/temp08topk30/'\n",
        "\n",
        "generatemusic(model, ids, block_size, 2000, 0.8, 30, folder_path, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lkoI2kAu9V2",
        "outputId": "2a8f411c-9d81-450c-d759-46281e8745f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icyS1lA1zFKD"
      },
      "source": [
        "Running Validation on the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSQnz2nLzEpb",
        "outputId": "c23106b3-6edf-483a-af20-38d891a2e3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1632008\n",
            "1632008\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "folder_path_valid = '/content/drive/Shareddrives/EC523_Project/CodeWorkspace/tokens_BPE_valid'\n",
        "\n",
        "block_size = 192\n",
        "ids, totlength = ConcatTokens(folder_path_valid, block_size)\n",
        "print(len(ids))\n",
        "print(totlength)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsDcjgxu0T_9"
      },
      "outputs": [],
      "source": [
        "dataset_valid = MakeDataset(ids, totlength, block_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFKASrrmzAyx"
      },
      "outputs": [],
      "source": [
        "# Validation Mode\n",
        "model.eval()\n",
        "data = dataset_valid\n",
        "loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                    batch_size=config.batch_size,\n",
        "                    num_workers=config.num_workers)\n",
        "# Disable gradient calculation to save memory\n",
        "with torch.no_grad():\n",
        "  pbar = tqdm(enumerate(loader), total=len(loader)) \n",
        "  for iter, (x, y) in pbar:\n",
        "    # place data on the correct device\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Forward pass on the validation dataset\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Calculate the loss between the predicted outputs and the ground truth labels\n",
        "    loss = torch.nn.functional.cross_entropy(outputs.logits.view(-1, outputs.logits.shape[-1]), y.view(-1))\n",
        "\n",
        "# Calculate the perplexity score from the validation loss\n",
        "perplexity = torch.exp(loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}